{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read training data from CSV file\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "tagged_sentences = train_data['tagged_sentence'].apply(eval).tolist() #Loading tagged sentence and making a list of list\n",
    "\n",
    "def get_states_obs(data):\n",
    "    observations = set()\n",
    "    states = set()\n",
    "    # count=0\n",
    "    for row in tagged_sentences:\n",
    "        for word, tag in row:\n",
    "            # count+=1                                      # Making a list of unique observations and states\n",
    "            observations.add(word)\n",
    "            states.add(tag)\n",
    "\n",
    "    # Convert sets to lists for consistency\n",
    "    observations = list(observations)\n",
    "    states = list(states)\n",
    "    # print(count)\n",
    "    return observations, states\n",
    "\n",
    "\n",
    "obs, states=get_states_obs(train_data)\n",
    "\n",
    "# Making a feature list for different possible suffixes in a word\n",
    "suffixes={'ed': 49, 'ing':50 ,'ly' : 51, 'tion' : 52, 'er': 53, 'or': 54, 'sion': 55, 'ful':56, 'less': 57, 'ment': 58, 'able': 59, 'ible': 60, 'ous':61, 'est':62, 'ify':63, 'ize': 64 }\n",
    "\n",
    "\n",
    "feature_matrix=[]\n",
    "\n",
    "def extract_suffix(word, suffixes):\n",
    "    if(len(word)>=5):\n",
    "        if(word[-2:] in suffixes):\n",
    "            # print(\"last2\")\n",
    "            return suffixes[word[-2:]]                  # Function to extract suffix from word\n",
    "    if(len(word)>=6):\n",
    "        if(word[-3:] in suffixes):\n",
    "            # print(\"last3\")\n",
    "            return suffixes[word[-3:]]\n",
    "    if(len(word)>=7):\n",
    "        if(word[-4:] in suffixes):\n",
    "            # print(\"last4\")\n",
    "            return suffixes[word[-4:]]\n",
    "    # print(\"no_suffix\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "# feature vector has first 0-48 features as distinct tags, 49-64 as suffix, 65 as first word, 66 as last word\n",
    "# Making features for the dataset\n",
    "overall_feature=[]\n",
    "for row in tagged_sentences:\n",
    "    for i in range(len(row)):\n",
    "        feature_vector=np.zeros((67))\n",
    "        word=row[i][0]\n",
    "        if(i==0):\n",
    "            feature_vector[65]=1\n",
    "        else:\n",
    "            prev_tag=row[i-1][1]\n",
    "            feature_vector[states.index(prev_tag)]=1\n",
    "        if(i+1==len(row)):\n",
    "            feature_vector[66]=1\n",
    "        suffix_index=extract_suffix(word, suffixes)\n",
    "        if(suffix_index!=0):\n",
    "            feature_vector[suffix_index]=1\n",
    "        overall_feature.append(feature_vector)\n",
    "\n",
    "overall_feature=np.array(overall_feature)       # Training set after converting it to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the model for multinomial logistic regression\n",
    "\n",
    "class MaximumEntropyMarkovModel:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z=np.array(z)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(42)  # Set seed for reproducibility\n",
    "        self.weights = np.random.randn(X.shape[1], y.shape[1])\n",
    "        self.bias = np.random.randn(1, y.shape[1])\n",
    "        for i in range(self.num_iterations):\n",
    "            for j in range(0, X.shape[0], self.batch_size):\n",
    "                # print('batch',j)\n",
    "                X_batch = X[j:j+self.batch_size]\n",
    "                y_batch = y[j:j+self.batch_size]\n",
    "                linear_model = (np.dot( X_batch, self.weights))\n",
    "                y_predicted = self.softmax(linear_model)\n",
    "        \n",
    "                dw = (1 / X_batch.shape[0]) * np.dot(X_batch.T, (y_predicted - y_batch))\n",
    "                db = (1 / X_batch.shape[0]) * np.sum(y_predicted - y_batch)\n",
    "        \n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.softmax(linear_model)\n",
    "        return np.argmax(y_predicted, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2828a\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pos_labels=[]\n",
    "for row in tagged_sentences:\n",
    "    for word, tag in row:\n",
    "        pos_labels.append(states.index(tag))\n",
    "\n",
    "\n",
    "pos_labels=np.array(pos_labels)\n",
    "# print(pos_labels.shape)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One hot encoding the target labels\n",
    "one_hot = OneHotEncoder(sparse=False)\n",
    "one_hot_output = one_hot.fit_transform(pos_labels.reshape(-1, 1))\n",
    "\n",
    "# print(one_hot_output.shape)\n",
    "# # # Create and train the model\n",
    "# # model = MultinomialLogisticRegression(learning_rate=0.01, num_iterations=10000)\n",
    "# # model.fit(X, y)\n",
    "\n",
    "# # # Predict the class labels of the same data (for demonstration purposes)\n",
    "# # y_pred = model.predict(X)\n",
    "# # print(y_pred)\n",
    "# # print(\"lundÂ lelo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=overall_feature\n",
    "y=one_hot_output\n",
    "\n",
    "# print(y.shape)\n",
    "# Training the Model\n",
    "model = MaximumEntropyMarkovModel(learning_rate=0.01, num_iterations=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "# sample_test=['For', 'you', 'have', 'been', 'reborn', ',', 'not', 'from', 'corruptible', 'seed', 'but', 'from', 'incorruptible', ',', 'through', 'the', 'word', 'of', 'God', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "# Function for making features for test dataset\n",
    "def make_feature_sentence(sentence, suffixes, word_index, prev_tag):\n",
    "    feature_vector=np.zeros(67)\n",
    "    suffix_index=extract_suffix(sentence[word_index], suffixes)\n",
    "    if(suffix_index!=0):\n",
    "        feature_vector[suffix_index]=1\n",
    "    if(word_index==0):\n",
    "        feature_vector[65]=1\n",
    "    else:\n",
    "        feature_vector[states.index(prev_tag)]=1\n",
    "    if(word_index+1==len(sentence)):\n",
    "        feature_vector[66]=1\n",
    "    return feature_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files=pd.read_csv(r\"C:\\Users\\2828a\\OneDrive\\Desktop\\ELL884 ASS1\\test_small.csv\")\n",
    "\n",
    "test_data=test_files['untagged_sentence'].apply(eval).tolist()\n",
    "\n",
    "# print(test_data[0])\n",
    "total_output=[]\n",
    "for sentence in test_data:\n",
    "    sent_output=[]\n",
    "    for i in range(len(sentence)):\n",
    "        if(i!=0):\n",
    "            feature=make_feature_sentence(sentence, suffixes, i, sent_output[i-1][1])\n",
    "        else:\n",
    "            feature=make_feature_sentence(sentence,suffixes, i, 0)\n",
    "        y_pred = model.predict(feature)\n",
    "        sent_output.append((sentence[i], states[y_pred[0]]))\n",
    "    total_output.append(sent_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.array(test_files['id'].to_list(), dtype=\"object\")\n",
    "\n",
    "def save_tagged_sentences_to_csv(tagged_sentences, ids, filename):\n",
    "    df = pd.DataFrame({'id': ids, 'tagged_sentence': tagged_sentences})\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename,index=False)\n",
    "\n",
    "output_path=r'C:\\Users\\2828a\\OneDrive\\Desktop\\ELL884 ASS1\\output_memm.csv'\n",
    "save_tagged_sentences_to_csv(total_output, ids, output_path)\n",
    "df = pd.read_csv(output_path)\n",
    "# print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
