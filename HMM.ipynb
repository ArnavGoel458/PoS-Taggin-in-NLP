{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read training data from CSV file\n",
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# # Extract untagged sentences and tagged sentences from the training data\n",
    "# untagged_sentences = train_data['untagged_sentence'].apply(eval).tolist()\n",
    "# tagged_sentences = train_data['tagged_sentence'].apply(eval).tolist()\n",
    "\n",
    "# # Flatten the lists to get individual words and POS tags\n",
    "# words = [word for sentence in untagged_sentences for word in sentence]\n",
    "# pos_tags = [pos_tag[1] for sentence_tags in tagged_sentences for pos_tag in sentence_tags]\n",
    "# sentences=[sentence for sentence in untagged_sentences]\n",
    "# tagged_sentences=[sentence for sentence in tagged_sentences]\n",
    "\n",
    "tagged_sentences = train_data['tagged_sentence'].apply(eval).tolist()\n",
    "\n",
    "def get_states_obs(data):\n",
    "    observations = set()\n",
    "    states = set()\n",
    "\n",
    "    for row in tagged_sentences:\n",
    "        for word, tag in row:\n",
    "            observations.add(word)\n",
    "            states.add(tag)\n",
    "\n",
    "    observations = list(observations)\n",
    "    states = list(states)\n",
    "    \n",
    "    return observations, states\n",
    "\n",
    "\n",
    "obs, states=get_states_obs(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "    def __init__(self, states, observations):\n",
    "        self.states = states\n",
    "        self.state_to_index = {state: i for i, state in enumerate(states)}\n",
    "        self.observations = observations\n",
    "        self.observation_to_index = {obs: i for i, obs in enumerate(observations)}\n",
    "        self.num_states = len(states)\n",
    "        self.num_observations = len(observations)\n",
    "        self.transition_probability = np.zeros((self.num_states, self.num_states))\n",
    "        self.emission_probability = np.zeros((self.num_states, self.num_observations))\n",
    "        self.initial_state_probabilities = np.zeros(self.num_states)\n",
    "        self.unknown_word_prob = 1e-5\n",
    "\n",
    "    def train(self, training_data):\n",
    "        # Estimate initial state probabilities\n",
    "        for sentence in training_data:\n",
    "            self.initial_state_probabilities[self.state_to_index[sentence[0][1]]] += 1\n",
    "\n",
    "        self.initial_state_probabilities /= np.sum(self.initial_state_probabilities)\n",
    "\n",
    "        # Estimate transition and emission probabilities\n",
    "        for sentence in training_data:\n",
    "            for i in range(len(sentence)):\n",
    "                current_state = self.state_to_index[sentence[i][1]]\n",
    "                if(i+1!=len(sentence)):\n",
    "                    next_state = self.state_to_index[sentence[i + 1][1]]\n",
    "                    self.transition_probability[current_state, next_state] += 1\n",
    "                \n",
    "                current_observation = self.observation_to_index[sentence[i][0]]\n",
    "                self.emission_probability[current_state, current_observation] += 1\n",
    "\n",
    "        # Laplace smoothing\n",
    "        self.transition_probability = (self.transition_probability) / (\n",
    "            np.sum(self.transition_probability, axis=1, keepdims=True))\n",
    "        self.emission_probability = (self.emission_probability) / (\n",
    "            np.sum(self.emission_probability, axis=1, keepdims=True))\n",
    "\n",
    "    def viterbi_algorithm(self, observation_sequence):\n",
    "        T = len(observation_sequence)\n",
    "        V = np.zeros((self.num_states, T))\n",
    "        B = np.zeros((self.num_states, T), dtype=int)\n",
    "\n",
    "        # Initialization step\n",
    "        V[:, 0] = self.initial_state_probabilities * self.get_emission_probabilities(observation_sequence[0])\n",
    "\n",
    "        # Recursion step\n",
    "        for t in range(1, T):\n",
    "            for s in range(self.num_states):\n",
    "                trans_prob = V[:, t - 1] * self.transition_probability[:, s]\n",
    "                max_trans_prob = np.max(trans_prob)\n",
    "                max_trans_prob_state = np.argmax(trans_prob)\n",
    "                V[s, t] = max_trans_prob * self.get_emission_probabilities(observation_sequence[t], state=s)\n",
    "                B[s, t] = max_trans_prob_state\n",
    "\n",
    "        # Termination step\n",
    "        best_path_prob = np.max(V[:, -1])\n",
    "        best_last_state = np.argmax(V[:, -1])\n",
    "\n",
    "        # Backtrack\n",
    "        best_path = [best_last_state]\n",
    "        for t in range(T - 1, 0, -1):\n",
    "            best_last_state = B[best_last_state, t]\n",
    "            best_path.insert(0, best_last_state)\n",
    "\n",
    "        return best_path, best_path_prob\n",
    "\n",
    "    def get_emission_probabilities(self, observation, state=None):\n",
    "        if state is not None:\n",
    "            if observation==len(self.observations):\n",
    "                return self.unknown_word_prob\n",
    "            else:\n",
    "                return self.emission_probability[state, observation] \n",
    "        else:\n",
    "            if(observation!=len(self.observations)):\n",
    "                return np.array([self.emission_probability[s, observation] for s in range(self.num_states)])\n",
    "            else:\n",
    "             return np.array([self.unknown_word_prob]*self.num_states)\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        sentence_indices =[]\n",
    "        for word in sentence:\n",
    "            if(word not in self.observations):\n",
    "                sentence_indices.append(len(self.observations))\n",
    "            else:\n",
    "                sentence_indices.append(self.observations.index(word))\n",
    "\n",
    "        best_path, best_path_prob = self.viterbi_algorithm(sentence_indices)\n",
    "\n",
    "        predicted_tags = [self.states[state] for state in best_path]\n",
    "        return predicted_tags, best_path_prob\n",
    "    \n",
    "\n",
    "# # Example usage:\n",
    "# # Define POS tags and vocabulary\n",
    "# states = ['Noun', 'Verb', 'Adjective']\n",
    "# observations = ['cat', 'dog', 'run', 'jump', 'quick', 'lazy']\n",
    "\n",
    "# # Define initial, transition, and emission probabilities\n",
    "# initial_prob = np.array([0.4, 0.4, 0.2])\n",
    "# transition_prob = np.array([[0.3, 0.4, 0.3],\n",
    "#                              [0.2, 0.5, 0.3],\n",
    "#                              [0.4, 0.3, 0.3]])\n",
    "# emission_prob = np.array([[0.5, 0.2, 0.1, 0.1, 0.05, 0.05],\n",
    "#                             [0.1, 0.2, 0.3, 0.1, 0.2, 0.1],\n",
    "#                             [0.1, 0.1, 0.2, 0.4, 0.1, 0.1]])\n",
    "\n",
    "# # Create HMM for POS tagging\n",
    "# hmm = HMM_POS_Tagging(states, observations, initial_prob, transition_prob, emission_prob)\n",
    "\n",
    "# # Example sentence for prediction\n",
    "# test_sentence = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "# # Predict POS tags using Viterbi algorithm\n",
    "# predicted_tags, likelihood = hmm.predict(test_sentence)\n",
    "\n",
    "# print(\"Predicted POS Tags:\", predicted_tags)\n",
    "# print(\"Likelihood of the sequence:\", likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm=HMM(states, obs)\n",
    "# test_data = pd.read_csv('test_small.csv')\n",
    "\n",
    "# # untagged_test_sentences = test_data['untagged_sentence'].apply(eval).tolist()\n",
    "\n",
    "# test_sentences=[]\n",
    "hmm.train(tagged_sentences)\n",
    "# sample=['For', 'you', 'have', 'been', 'reborn', ',', 'not', 'from', 'corruptible', 'seed', 'but', 'from', 'incorruptible', ',', 'through', 'the', 'word', 'of', 'God', '.']\n",
    "test_data=pd.read_csv('test_small.csv')\n",
    "output_list=[]\n",
    "\n",
    "test_sentences=test_data[\"untagged_sentence\"].apply(eval).tolist()\n",
    "for sentence in test_sentences:\n",
    "    best_path,best_path_prob=hmm.predict(sentence)\n",
    "    temp_list=[]\n",
    "    for i in range(len(sentence)):\n",
    "        # temp_var=False\n",
    "        # for t in sentence[i]:\n",
    "        #     if(65<=ord(t)<=90 or 97<=ord(t)<=122 or 48<=ord(t)<=57):\n",
    "        #         temp_var=True\n",
    "        # if(temp_var==False):\n",
    "        #     temp_tuple=(sentence[i], sentence[i])\n",
    "        # else:\n",
    "        temp_tuple=(sentence[i], best_path[i])\n",
    "        temp_list.append(temp_tuple)\n",
    "    output_list.append(temp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('For', 'CS'), ('you', 'PP'), ('have', 'HV'), ('been', 'BE'), ('reborn', 'VB'), (',', ','), ('not', '*'), ('from', 'IN'), ('corruptible', 'JJ'), ('seed', 'NN'), ('but', 'CC'), ('from', 'IN'), ('incorruptible', 'JJ'), (',', ','), ('through', 'IN'), ('the', 'AT'), ('word', 'NN'), ('of', 'IN'), ('God', 'NP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "ids = np.array(test_data['id'].to_list(), dtype=\"object\")\n",
    "\n",
    "print(output_list[0])\n",
    "def save_tagged_sentences_to_csv(tagged_sentences, ids, filename):\n",
    "    df = pd.DataFrame({'id': ids, 'tagged_sentence': tagged_sentences})\n",
    "\n",
    "    df.to_csv(filename,index=False)\n",
    "\n",
    "output_path=r'C:\\Users\\2828a\\OneDrive\\Desktop\\ELL884 ASS1\\output.csv'\n",
    "save_tagged_sentences_to_csv(output_list, ids, output_path)\n",
    "df = pd.read_csv(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
